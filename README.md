# âœ¨ Next-Gen Multimodal Sign Language Recognition Platform ğŸ¤Ÿ

This project presents a real-time AI-powered Sign Language Recognition system that translates hand gestures into readable text and speech using deep learning and computer vision techniques.

The platform is designed to bridge the communication gap between hearing-impaired individuals and the general public by enabling real-time gesture-to-text and text-to-speech conversion.

---

## ğŸ“Œ Problem Statement

Communication between hearing-impaired individuals and non-sign language users can be challenging. Most people are not trained in sign language, which creates communication barriers in education, healthcare, workplaces, and daily life.

An intelligent system is required to:
- Detect hand gestures in real time
- Convert gestures into readable text
- Convert text into audible speech
- Ensure accurate and stable predictions

---

## ğŸ¯ Objectives

- Implement real-time sign language detection using a webcam
- Develop separate AI models for Letters (Aâ€“Z), Numbers (0â€“9), and Words
- Improve prediction stability using confidence thresholds and delay control
- Convert predicted text into speech
- Provide a simple, user-friendly graphical interface

---

## ğŸ§  Methodology

The system follows these steps:

1. Capture live webcam video using OpenCV
2. Detect hand landmarks using MediaPipe
3. Extract 21 keypoints from the detected hand
4. Normalize keypoints (for letters and words)
5. Pass keypoints into trained deep learning models
6. Apply confidence threshold filtering
7. Display predicted text in the GUI
8. Convert predicted text into speech using pyttsx3

---

## ğŸ¤– Models Used

### ğŸ”¤ Letters Model
- 26 Classes (Aâ€“Z)
- Keypoint normalization applied
- Dense Neural Network
- Confidence threshold-based filtering

### ğŸ”¢ Numbers Model
- 10 Classes (0â€“9)
- Raw landmark coordinates used (as per training format)
- Dense Neural Network
- Delay mechanism for stable prediction

### ğŸ“ Words Model
- Sequence-based prediction (30 frames)
- LSTM Sequential Model
- Used for dynamic gesture recognition

---

## âš™ï¸ Features

- ğŸ”¤ Alphabet Recognition (Aâ€“Z)
- ğŸ”¢ Number Recognition (0â€“9)
- ğŸ“ Word Recognition
- ğŸ¥ Real-time Webcam Detection
- ğŸ”Š Text-to-Speech Conversion
- â¯ Start / Pause / Stop Camera Controls
- ğŸ¯ Mode Selection (Letters / Numbers / Words)
- âœ Delete / Clear / New Line Support
- ğŸ–¥ User-Friendly GUI using Tkinter

---

## ğŸ“Š System Configuration

- Window Size: 1200 x 720
- Camera Resolution: 800 x 450
- Letter Confidence Threshold: 0.7
- Number Confidence Threshold: 0.75
- Word Confidence Threshold: 0.8
- Word Sequence Length: 30 Frames

---

## ğŸ›  Technologies Used

- Python
- OpenCV
- MediaPipe
- TensorFlow / Keras
- NumPy
- Tkinter
- pyttsx3

---

## ğŸš€ Applications

- Assistive communication systems
- Smart education platforms
- Inclusive AI solutions
- Healthcare support systems
- Real-time gesture-based interaction systems

---

## ğŸ“Œ Future Improvements

- Improve number prediction accuracy
- Add sentence-level grammar correction
- Deploy as a web application
- Add multilingual speech output
- Improve dataset size for better model generalization

---

â­ This project demonstrates the practical implementation of Deep Learning and Computer Vision for building inclusive and accessible AI systems.
